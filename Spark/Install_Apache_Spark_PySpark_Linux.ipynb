{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><span style =\"color:black;font-weight:bold\">Install PySpark on Linux </span> </h1>\n",
    "<p></p><p></p>Youtube tutorial available at: https://www.youtube.com/watch?v=I5JtvpyM14U\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style =\"color:black;font-weight:bold;font-size: 18px;\">Download Spark</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Go to the Apache Spark website. \n",
    "<h5 align=\"center\"> http://spark.apache.org/downloads.html </h5>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Download_linux.png\" height=\"340\" width=\"340\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul> \n",
    "    <li> a) Choose a Spark release (I prefer 2.0.0)</li>\n",
    "    <li> b) Choose a package type: (this installation prefers \"Pre-built for Hadoop 2.7 and later\")</li> \n",
    "    <li> c) Choose a download type: (Direct Download) </li>\n",
    "    <li> d) Download Spark: http://d3kbcqa49mib13.cloudfront.net/spark-2.0.0-bin-hadoop2.7.tgz\n",
    "    </li> (you can click on this or go to http://spark.apache.org/downloads.html to choose your own Spark Version. \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Make sure you have java installed on your machine. If you don't, I found the link below useful, but feel free to use something else. \n",
    "\n",
    "\n",
    "http://tecadmin.net/install-oracle-java-8-jdk-8-ubuntu-via-ppa/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Go to your home directory. (You can use the command in red)\n",
    "\n",
    "<span style =\"color:red;\"> cd ~ </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) Unzip the folder in your home directory using the following command. \n",
    "\n",
    "<span style =\"color:red;\"> tar -zxvf spark-2.0.0-bin-hadoop2.7.tgz </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Use the following command to see that you have a .bash_profile \n",
    "\n",
    "<span style =\"color:red;\"> ls -a </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Next, we will edit our .bashrc so we can open a spark notebook in any directory\n",
    "\n",
    "<span style =\"color:red;\"> nano .bashrc </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/bashrc.png\" height=\"340\" width=\"340\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) Don't remove anything in your .bashrc file. Add the following to the bottom of your .bashrc file <br>\n",
    "\n",
    "function snotebook () <br>\n",
    "{\n",
    "\t#Spark path (based on your computer)\n",
    "\tSPARK_PATH=~/spark-2.0.0-bin-hadoop2.7\n",
    "\n",
    "\texport PYSPARK_DRIVER_PYTHON=\"jupyter\"\n",
    "\texport PYSPARK_DRIVER_PYTHON_OPTS=\"notebook\"\n",
    "\n",
    "\t$SPARK_PATH/bin/pyspark --master local[2]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) Save and exit out of your .bashrc file. \n",
    "Either close the terminal and open a new one or in your terminal type: \n",
    "\n",
    "<span style =\"color:red;\"> source .bashrc </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) Done! To test out PySpark please continue to the next tutorial or continue to step 10\n",
    "\n",
    "<b>Word Count Youtube: </b> <br>\n",
    "https://www.youtube.com/watch?v=jg7Z8ctKpEs <br>\n",
    "<b>Word Count Code:<br> </b>https://github.com/mGalarnyk/Python_Tutorials/blob/master/PySpark_Basics/PySpark_Part1_Word_Count_Removing_Punctuation_Pride_Prejudice.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Optional Test:</b> Monte Carlo experiment to find an estimate pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "TOTAL = 1000000\n",
    "dots = sc.parallelize([2.0 * np.random.random(2) - 1.0\n",
    "                       for i in range(TOTAL)]).cache()\n",
    "plt.figure(figsize = (10, 5))\n",
    "\n",
    "# Plot 1\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.xlim((-1.0, 1.0))\n",
    "plt.ylim((-1.0, 1.0))\n",
    "\n",
    "sample = dots.sample(False, 0.01)\n",
    "X = sample.map(itemgetter(0)) \\\n",
    "          .collect()\n",
    "Y = sample.map(itemgetter(1)) \\\n",
    "          .collect()\n",
    "plt.scatter(X, Y)\n",
    "\n",
    "# Plot 2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.xlim((-1.0, 1.0))\n",
    "plt.ylim((-1.0, 1.0))\n",
    "\n",
    "inCircle = lambda v: np.linalg.norm(v) <= 1.0\n",
    "dotsIn = sample.filter(inCircle) \\\n",
    "               .cache()\n",
    "dotsOut = sample.filter(lambda v: not inCircle(v)) \\\n",
    "                .cache()\n",
    "\n",
    "# inside circle\n",
    "Xin = dotsIn.map(itemgetter(0)) \\\n",
    "            .collect()\n",
    "Yin = dotsIn.map(itemgetter(1)) \\\n",
    "            .collect()\n",
    "plt.scatter(Xin, Yin, color = 'r')\n",
    "\n",
    "# outside circle\n",
    "Xout = dotsOut.map(itemgetter(0)) \\\n",
    "              .collect()\n",
    "Yout = dotsOut.map(itemgetter(1)) \\\n",
    "              .collect()\n",
    "plt.scatter(Xout, Yout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style =\"color:black;font-weight:bold;font-size: 18px;\">Notes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The PYSPARK_DRIVER_PYTHON parameter and the PYSPARK_DRIVER_PYTHON_OPTS parameter are used to launch the PySpark shell in Jupyter Notebook. The --master parameter is used for setting the master node address. Here we launch Spark locally on 2 cores for local testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style =\"color:black;font-weight:bold;font-size: 18px;\">For Python 3 Users</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/Python3.png\" height=\"340\" width=\"340\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have to add the line in red before you use alias snotebook='$SPARK_PATH/bin/pyspark --master local[2]' line or you will get the error in the image above. \n",
    "<span style =\"color:red;\"> <br>\n",
    "export PYSPARK_PYTHON=python3\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style =\"color:black;font-weight:bold;font-size: 18px;\">Other useful PySpark tutorials</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.dataquest.io/blog/installing-pyspark/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
